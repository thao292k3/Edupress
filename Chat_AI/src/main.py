import streamlit as st
import os
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_classic.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# 1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
storage_path = os.path.join(root_dir, 'storage')

st.set_page_config(page_title="EduPress AI", page_icon="üéì")
st.title("ü§ñ EduPress AI Assistant")

# 2. ƒê·ªãnh nghƒ©a c√°ch AI tr·∫£ l·ªùi (Prompt Ti·∫øng Vi·ªát)
template = """B·∫°n l√† tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa trung t√¢m EduPress. 
D·ª±a v√†o ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c nh·∫•t.
N·∫øu c√¢u h·ªèi kh√¥ng c√≥ trong ng·ªØ c·∫£nh, h√£y l·ªãch s·ª± t·ª´ ch·ªëi.

Ng·ªØ c·∫£nh: {context}
C√¢u h·ªèi: {question}
Tr·∫£ l·ªùi:"""

QA_CHAIN_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=template,
)

# 3. Kh·ªüi t·∫°o AI
@st.cache_resource
def init_bot():
    # Model gemma2:2b ƒë·ªìng b·ªô v·ªõi file ingest.py
    embeddings = OllamaEmbeddings(model="gemma2:2b") 
    
    # K·∫øt n·ªëi t·ªõi Vector Database ƒë√£ t·∫°o t·ª´ ingest.py
    vectorstore = Chroma(
        persist_directory=storage_path, 
        embedding_function=embeddings
    )
    
    llm = ChatOllama(model="gemma2:2b", temperature=0)
    
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        # TƒÉng k=6 ƒë·ªÉ AI c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c to√†n b·ªô 6 kh√≥a h·ªçc c√πng l√∫c
        retriever=vectorstore.as_retriever(search_kwargs={"k": 6}),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )

# 4. Giao di·ªán Chat
if os.path.exists(storage_path):
    try:
        bot = init_bot()
        
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("H·ªèi v·ªÅ kh√≥a h·ªçc (vd: Kh√≥a h·ªçc Android gi√° bao nhi√™u?)"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("ƒêang suy nghƒ©..."):
                    # S·ª≠ d·ª•ng invoke ƒë·ªÉ ƒë·∫£m b·∫£o chu·∫©n LangChain m·ªõi
                    response = bot.invoke({"query": prompt})
                    answer = response["result"]
                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
    except Exception as e:
        st.error(f"L·ªói: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i Ollama ho·∫∑c model gemma2:2b.")
else:
    st.warning("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a c√≥ d·ªØ li·ªáu. H√£y ch·∫°y file ingest.py tr∆∞·ªõc!")